# Awesome Vision-and-Language Pre-Training[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
  <img width="250" src="https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667" "Awesome!">
</p>

A curated list of vision-and-language pre-training. :-)

## Contributing
Please feel free to send me [pull requests](https://github.com/zhjohnchan/awesome-pretraining-in-nlp/pulls) or email (chihung.chan@outlook.com) to add links.

## Table of Contents
- [Papers](#papers)
  - [Survey](#survey)
  - [Research Paper](#research-paper)
    - [Dual Encoders](#dual-encoders)
    - [Fusion Encoders](#fusion-encoders)
- [Datasets](#datasets)

## Papers
### Survey

### Research Paper
#### Dual Encoders

#### Fusion Encoders

## Datasets
| Dataset           | Images | Image-Text Pairs | Duration (hrs) | Note                                                                                                                                                                         |
|-------------------|--------|------------------|----------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| SBU               | 875k   | 875k             | -              | [reference](https://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs), [website](https://www.cs.virginia.edu/~vicente/sbucaptions/) |
| FLIKR             | 29k    | 145k             | -              | [reference](https://aclanthology.org/Q14-1006), [website](http://hockenmaier.cs.illinois.edu/DenotationGraph/)                                                               |
| COCO              | 113k   | 567k             | -              | [reference](https://arxiv.org/abs/1405.0312), [website](https://cocodataset.org/)                                                                                            |
| VG                | 108k   | 5.4m             | -              | [reference](https://arxiv.org/abs/1602.07332), [website](https://visualgenome.org/)                                                                                          |
| VGQA              | 108k   | 1.8m             | -              | [reference](https://arxiv.org/abs/1602.07332), [website](https://visualgenome.org/)                                                                                          |
| VQA               | 83k    | 444k             | -              | [reference](https://arxiv.org/abs/1612.00837), [website](https://visualqa.org/)                                                                                              |
| GQA               | 82k    | 1m               | -              | [reference](https://arxiv.org/abs/1902.09506), [website](https://cs.stanford.edu/people/dorarad/gqa/about.html)                                                              |
| CC3M              | 3m     | 3m               | -              | [reference](https://aclanthology.org/P18-1238), [website](https://ai.google.com/research/ConceptualCaptions/download)                                                        |
| CC12M             | 12m    | 12m              | -              | [reference](https://arxiv.org/abs/2102.08981), [website](https://github.com/google-research-datasets/conceptual-12m)                                                         |
| YFCC-15M          | 15m    | 15m              | -              | [reference](https://arxiv.org/abs/1503.01817), [website](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/)                                                     |
| WebImageText      | 400m   | 400m             | -              | [reference](https://arxiv.org/abs/2103.00020)                                                                                                                                |
| LAION-400M        | 400m   | 400m             | -              | [website](https://laion.ai/laion-400-open-dataset)                                                                                                                           |
| LAION-2B          | 2b     | 2b               | -              | [website](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)                                                                                     |
| RedCaps           | 12m    | 12m              |                | [reference](https://arxiv.org/abs/2111.11431), [website](https://redcaps.xyz/)                                                                                               |
| AltText           | 1.8b   | 1.8b             | -              | [reference](https://arxiv.org/abs/2102.05918)                                                                                                                                |
| ImageNet-Captions | 464k   | 464k             | -              | [reference](https://arxiv.org/abs/2205.01397), [website](https://github.com/mlfoundations/imagenet-captions)                                                                 |
| Kinetics          | -      | -                | 1.4k           | [reference](https://arxiv.org/abs/1705.06950), [website](https://github.com/cvdfoundation/kinetics-dataset)                                                                  |
| TVQA              | -      | -                | 0.4k           | [reference](https://arxiv.org/abs/1809.01696), [website](https://tvqa.cs.unc.edu/)                                                                                           |
| HT100M            | -      | -                | 134k           | [reference](https://arxiv.org/abs/1906.03327), [website](https://www.di.ens.fr/willow/research/howto100m/)                                                                   |
| WebVid2M          | -      | -                | 13k            | [reference](https://arxiv.org/abs/2104.00650), [website](https://m-bain.github.io/webvid-dataset/)                                                                           |


## Licenses

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Zhihong Chen](https://github.com/zhjohnchan) has waived all copyright and related or neighboring rights to this work.
