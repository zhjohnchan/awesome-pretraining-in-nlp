# Awesome Vision-and-Language Pre-Training[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
  <img width="250" src="https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667" "Awesome!">
</p>

A curated list of vision-and-language pre-training. :-)

## Contributing
Please feel free to send me [pull requests](https://github.com/zhjohnchan/awesome-pretraining-in-nlp/pulls) or email (chihung.chan@outlook.com) to add links.

## Table of Contents
- [Papers](#papers)
  - [Survey](#survey)
  - [Research Paper](#research-paper)
    - [Dual Encoders](#dual-encoders)
    - [Fusion Encoders](#fusion-encoders)
- [Datasets](#datasets)

## Papers
### Survey

### Research Paper
#### Dual Encoders

#### Fusion Encoders
| Method         | Venue        | Reference                                                                                        | Authors                                                                                                                                                    |
|----------------|--------------|--------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **2019**       |              |                                                                                                  |                                                                                                                                                            |
| VisualBERT     | Arxiv-2019   | VisualBERT: A Simple and Performant Baseline for Vision and Language                             | Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang                                                                                      |
| ViLBERT        | NeurIPS-2019 | ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks | Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee                                                                                                            |
| LXMERT         | EMNLP-2019   | LXMERT: Learning Cross-Modality Encoder Representations from Transformers                        | Hao Tan, Mohit Bansal                                                                                                                                      |
| **2020**       |              |                                                                                                  |                                                                                                                                                            |
| ImageBERT      | Arxiv-2020   | ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data             | Di Qi, Lin Su, Jia Song, Edward Cui, Taroon Bharti, Arun Sacheti                                                                                           |
| InterBERT      | Arxiv-2020   | InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining                           | Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia Yang                                                                                   |
| PixelBERT      | Arxiv-2020   | Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers                     | Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, Jianlong Fu                                                                                            |
| UNITER         | ECCV-2020    | UNITER: UNiversal Image-TExt Representation Learning                                             | Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu                                                        |
| VisDial-BERT   | ECCV-2020    | Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline                    | Vishvak Murahari, Dhruv Batra, Devi Parikh, Abhishek Das                                                                                                   |
| OSCAR          | ECCV-2020    | Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks                           | Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao               |
| Unicoder-VL    | AAAI-2020    | Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training             | Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, Ming Zhou                                                                                          |
| VLP            | AAAI-2020    | Unified Vision-Language Pre-Training for Image Captioning and VQA                                | Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, Jianfeng Gao                                                                            |
| ERNIE-ViL      | AAAI-2021    | ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph                | Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang                                                                                    |
| VL-BERT        | ICLR-2020    | VL-BERT: Pre-training of Generic Visual-Linguistic Representations                               | Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai                                                                                     |
| 12-IN-1        | CVPR-2020    | 12-in-1: Multi-Task Vision and Language Representation Learning                                  | Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, Stefan Lee                                                                                       |
| VILLA          | NeurIPS-2020 | Large-Scale Adversarial Training for Vision-and-Language Representation Learning                 | Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu                                                                                        |
| **2021**       |              |                                                                                                  |                                                                                                                                                            |
| VLMO           | Arixv-2021   | VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts                      | Wenhui Wang, Hangbo Bao, Li Dong, Furu Wei                                                                                                                 |
| XGPT           | NLPCC-2021   | XGPT: Cross-modal Generative Pre-Training for Image Captioning                                   | Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou                                   |
| VL-T5          | ICML-2021    | Unifying Vision-and-Language Tasks via Text Generation                                           | Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal                                                                                                             |
| ViLT           | ICML-2021    | ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision                  | Wonjae Kim, Bokyung Son, Ildoo Kim                                                                                                                         |
| Visual Parsing | NeurIPS-2021 | Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training      | Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, Jiebo Luo                                                                        |
| ALBEF          | NeurIPS-2021 | Align before Fuse: Vision and Language Representation Learning with Momentum Distillation        | Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, Steven Hoi                                                       |
| E2E-VLP        | ACL-2021     | E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning                     | Haiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, Fei Huang                                                                        |
| SOHO           | CVPR-2021    | Seeing Out of tHe bOx: End-to-End Pre-training for Vision-Language Representation Learning       | Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, Jianlong Fu                                                                               |
| VLN-BERT       | CVPR-2021    | A Recurrent Vision-and-Language BERT for Navigation                                              | Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, Stephen Gould                                                                                    |
| VinVL          | CVPR-2021    | VinVL: Revisiting Visual Representations in Vision-Language Models                               | Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao                                                     |
| SimVLM         | ICLR-2021    | SimVLM: Simple Visual Language Model Pretraining with Weak Supervision                           | Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao                                                                                  |
| **2022**       |              |                                                                                                  |                                                                                                                                                            |
| METER          | CVPR-2022    | An Empirical Study of Training End-to-End Vision-and-Language Transformers                       | Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, Michael Zeng |
| CLIP-ViL       | ICLR-2022    | How Much Can CLIP Benefit Vision-and-Language Tasks?                                             | Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer                                               |

## Datasets
| Dataset Name      | Images | Image-Text Pairs | Duration (hrs) | Note                                                                                                                                                                         |
|-------------------|-------:|-----------------:|---------------:|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| SBU               |   875k |             875k |              - | [reference](https://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs), [website](https://www.cs.virginia.edu/~vicente/sbucaptions/) |
| FLIKR             |    29k |             145k |              - | [reference](https://aclanthology.org/Q14-1006), [website](http://hockenmaier.cs.illinois.edu/DenotationGraph/)                                                               |
| COCO              |   113k |             567k |              - | [reference](https://arxiv.org/abs/1405.0312), [website](https://cocodataset.org/)                                                                                            |
| VG                |   108k |             5.4m |              - | [reference](https://arxiv.org/abs/1602.07332), [website](https://visualgenome.org/)                                                                                          |
| VGQA              |   108k |             1.8m |              - | [reference](https://arxiv.org/abs/1602.07332), [website](https://visualgenome.org/)                                                                                          |
| VQA               |    83k |             444k |              - | [reference](https://arxiv.org/abs/1612.00837), [website](https://visualqa.org/)                                                                                              |
| GQA               |    82k |               1m |              - | [reference](https://arxiv.org/abs/1902.09506), [website](https://cs.stanford.edu/people/dorarad/gqa/about.html)                                                              |
| CC3M              |     3m |               3m |              - | [reference](https://aclanthology.org/P18-1238), [website](https://ai.google.com/research/ConceptualCaptions/download)                                                        |
| CC12M             |    12m |              12m |              - | [reference](https://arxiv.org/abs/2102.08981), [website](https://github.com/google-research-datasets/conceptual-12m)                                                         |
| YFCC-15M          |    15m |              15m |              - | [reference](https://arxiv.org/abs/1503.01817), [website](https://multimediacommons.wordpress.com/yfcc100m-core-dataset/)                                                     |
| WebImageText      |   400m |             400m |              - | [reference](https://arxiv.org/abs/2103.00020)                                                                                                                                |
| LAION-400M        |   400m |             400m |              - | [website](https://laion.ai/laion-400-open-dataset)                                                                                                                           |
| LAION-2B          |     2b |               2b |              - | [website](https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/)                                                                                     |
| RedCaps           |    12m |              12m |                | [reference](https://arxiv.org/abs/2111.11431), [website](https://redcaps.xyz/)                                                                                               |
| AltText           |   1.8b |             1.8b |              - | [reference](https://arxiv.org/abs/2102.05918)                                                                                                                                |
| ImageNet-Captions |   464k |             464k |              - | [reference](https://arxiv.org/abs/2205.01397), [website](https://github.com/mlfoundations/imagenet-captions)                                                                 |
| Kinetics          |      - |                - |           1.4k | [reference](https://arxiv.org/abs/1705.06950), [website](https://github.com/cvdfoundation/kinetics-dataset)                                                                  |
| TVQA              |      - |                - |           0.4k | [reference](https://arxiv.org/abs/1809.01696), [website](https://tvqa.cs.unc.edu/)                                                                                           |
| HT100M            |      - |                - |           134k | [reference](https://arxiv.org/abs/1906.03327), [website](https://www.di.ens.fr/willow/research/howto100m/)                                                                   |
| WebVid2M          |      - |                - |            13k | [reference](https://arxiv.org/abs/2104.00650), [website](https://m-bain.github.io/webvid-dataset/)                                                                           |

## Licenses

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Zhihong Chen](https://github.com/zhjohnchan) has waived all copyright and related or neighboring rights to this work.
