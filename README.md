# Awesome Pre-training in NLP[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<p align="center">
  <img width="250" src="https://camo.githubusercontent.com/1131548cf666e1150ebd2a52f44776d539f06324/68747470733a2f2f63646e2e7261776769742e636f6d2f73696e647265736f726875732f617765736f6d652f6d61737465722f6d656469612f6c6f676f2e737667" "Awesome!">
</p>

A curated list of pre-training in NLP. :-)

## Contributing
Please feel free to send me [pull requests](https://github.com/zhjohnchan/awesome-pretraining-in-nlp/pulls) or email (chihung.chan@outlook.com) to add links.

## Table of Contents
- [Papers](#papers)
  - [Survey](#survey)
  - [Research Paper](#research-paper)

## Papers
### Research Paper
|   Year | Venue    | Title                                                                                                                                                                                        |
|-------:|:---------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
|   2015 | ACL      | [Pre-training of Hidden-Unit CRFs](https://aclanthology.org/P15-2032.pdf)                                                                                                                    |
|   2017 | ACL      | [Neural Word Segmentation with Rich Pretraining](https://aclanthology.org/P17-1078.pdf)                                                                                                      |
|   2017 | EMNLP    | [Unsupervised Pretraining for Sequence to Sequence Learning](https://aclanthology.org/D17-1039.pdf)                                                                                          |
|   2017 | CoNLL    | [TurkuNLP: Delexicalized Pre-training of Word Embeddings for Dependency Parsing](https://aclanthology.org/K17-3012.pdf)                                                                      |
|   2018 | ACL      | [Pretraining Sentiment Classifiers with Unlabeled Dialog Data](https://aclanthology.org/P18-2121.pdf)                                                                                        |
|   2018 | ACL      | [Sub-word information in pre-trained biomedical word representations: evaluation and hyper-parameter optimization](https://aclanthology.org/W18-2307.pdf)                                    |
|   2018 | EMNLP    | [An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing](https://aclanthology.org/D18-1291.pdf)                       |
|   2018 | EMNLP    | [Code-switched Language Models Using Dual RNNs and Same-Source Pretraining](https://aclanthology.org/D18-1346.pdf)                                                                           |
|   2018 | EMNLP    | [Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation](https://aclanthology.org/D18-1520.pdf)                                                                          |
|   2018 | NAACL    | [When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?](https://aclanthology.org/N18-2084.pdf)                                                                 |
|   2019 | ACL      | [Extracting Multiple-Relations in One-Pass with Pre-Trained Transformers](https://aclanthology.org/P19-1132.pdf)                                                                             |
|   2019 | ACL      | [Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction](https://aclanthology.org/P19-1134.pdf)                                                     |
|   2019 | ACL      | [Gender-preserving Debiasing for Pre-trained Word Embeddings](https://aclanthology.org/P19-1160.pdf)                                                                                         |
|   2019 | ACL      | [Topic Sensitive Attention on Generic Corpora Corrects Sense Bias in Pretrained Embeddings](https://aclanthology.org/P19-1168.pdf)                                                           |
|   2019 | ACL      | [Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension](https://aclanthology.org/P19-1226.pdf)                                                |
|   2019 | ACL      | [Multilingual Constituency Parsing with Self-Attention and Pre-Training](https://aclanthology.org/P19-1340.pdf)                                                                              |
|   2019 | ACL      | [Pretraining Methods for Dialog Context Representation Learning](https://aclanthology.org/P19-1373.pdf)                                                                                      |
|   2019 | ACL      | [Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling](https://aclanthology.org/P19-1439.pdf)                                                  |
|   2019 | ACL      | [HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization](https://aclanthology.org/P19-1499.pdf)                                           |
|   2019 | ACL      | [Exploring Pre-trained Language Models for Event Extraction and Generation](https://aclanthology.org/P19-1522.pdf)                                                                           |
|   2019 | ACL      | [Variational Pretraining for Semi-supervised Text Classification](https://aclanthology.org/P19-1590.pdf)                                                                                     |
|   2019 | ACL      | [Unsupervised Pretraining for Neural Machine Translation Using Elastic Weight Consolidation](https://aclanthology.org/P19-2017.pdf)                                                          |
|   2019 | ACL      | [Multilingual Named Entity Recognition Using Pretrained Embeddings, Attention Mechanism and NCRF](https://aclanthology.org/W19-3713.pdf)                                                     |
|   2019 | ACL      | [Transfer Learning from Pre-trained BERT for Pronoun Resolution](https://aclanthology.org/W19-3812.pdf)                                                                                      |
|   2019 | ACL      | [To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks](https://aclanthology.org/W19-4302.pdf)                                                                        |
|   2019 | ACL      | [An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection](https://aclanthology.org/W19-4317.pdf)                                                                  |
|   2019 | ACL      | [Leveraging Pre-Trained Embeddings for Welsh Taggers](https://aclanthology.org/W19-4332.pdf)                                                                                                 |
|   2019 | ACL      | [A Neural Grammatical Error Correction System Built On Better Pre-training and Sequential Transfer Learning](https://aclanthology.org/W19-4423.pdf)                                          |
|   2019 | ACL      | [Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data](https://aclanthology.org/W19-4427.pdf)                                                        |
|   2019 | ACL      | [Identification of Adjective-Noun Neologisms using Pretrained Language Models](https://aclanthology.org/W19-5116.pdf)                                                                        |
|   2019 | ACL      | [A Comparison on Fine-grained Pre-trained Embeddings for the WMT19Chinese-English News Translation Task](https://aclanthology.org/W19-5324.pdf)                                              |
|   2019 | ACL      | [Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings](https://aclanthology.org/W19-5410.pdf)                                                             |
|   2019 | EMNLP    | [Explicit Cross-lingual Pre-training for Unsupervised Machine Translation](https://aclanthology.org/D19-1071.pdf)                                                                            |
|   2019 | EMNLP    | [Commonsense Knowledge Mining from Pretrained Models](https://aclanthology.org/D19-1109.pdf)                                                                                                 |
|   2019 | EMNLP    | [Robust Navigation with Language Pretraining and Stochastic Sampling](https://aclanthology.org/D19-1159.pdf)                                                                                 |
|   2019 | EMNLP    | [Unicoder: A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks](https://aclanthology.org/D19-1252.pdf)                                                            |
|   2019 | EMNLP    | [Delta-training: Simple Semi-Supervised Text Classification using Pretrained Word Embeddings](https://aclanthology.org/D19-1347.pdf)                                                         |
|   2019 | EMNLP    | [Harnessing Pre-Trained Neural Networks with Rules for Formality Style Transfer](https://aclanthology.org/D19-1365.pdf)                                                                      |
|   2019 | EMNLP    | [SciBERT: A Pretrained Language Model for Scientific Text](https://aclanthology.org/D19-1371.pdf)                                                                                            |
|   2019 | EMNLP    | [Pretrained Language Models for Sequential Sentence Classification](https://aclanthology.org/D19-1383.pdf)                                                                                   |
|   2019 | EMNLP    | [Text Summarization with Pretrained Encoders](https://aclanthology.org/D19-1387.pdf)                                                                                                         |
|   2019 | EMNLP    | [Combining Unsupervised Pre-training and Annotator Rationales to Improve Low-shot Text Classification](https://aclanthology.org/D19-1401.pdf)                                                |
|   2019 | EMNLP    | [Denoising based Sequence-to-Sequence Pre-training for Text Generation](https://aclanthology.org/D19-1412.pdf)                                                                               |
|   2019 | EMNLP    | [Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations](https://aclanthology.org/D19-1533.pdf)                                                            |
|   2019 | EMNLP    | [Cloze-driven Pretraining of Self-attention Networks](https://aclanthology.org/D19-1539.pdf)                                                                                                 |
|   2019 | EMNLP    | [Pre-Training BERT on Domain Resources for Short Answer Grading](https://aclanthology.org/D19-1628.pdf)                                                                                      |
|   2019 | EMNLP    | [UER: An Open-Source Toolkit for Pre-training Models](https://aclanthology.org/D19-3041.pdf)                                                                                                 |
|   2019 | EMNLP    | [Pretrained Ensemble Learning for Fine-Grained Propaganda Detection](https://aclanthology.org/D19-5020.pdf)                                                                                  |
|   2019 | EMNLP    | [Hello, It’s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems](https://aclanthology.org/D19-5602.pdf)                            |
|   2019 | EMNLP    | [Recycling a Pre-trained BERT Encoder for Neural Machine Translation](https://aclanthology.org/D19-5603.pdf)                                                                                 |
|   2019 | EMNLP    | [Biomedical relation extraction with pre-trained language representations and minimal task-specific architecture](https://aclanthology.org/D19-5713.pdf)                                     |
|   2019 | EMNLP    | [Extractive NarrativeQA with Heuristic Pre-Training](https://aclanthology.org/D19-5823.pdf)                                                                                                  |
|   2019 | EMNLP    | [Generalizing Question Answering System with Pre-trained Language Model Fine-tuning](https://aclanthology.org/D19-5827.pdf)                                                                  |
|   2019 | EMNLP    | [D-NET: A Pre-Training and Fine-Tuning Framework for Improving the Generalization of Machine Reading Comprehension](https://aclanthology.org/D19-5828.pdf)                                   |
|   2019 | EMNLP    | [Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks](https://aclanthology.org/D19-6011.pdf)       |
|   2019 | EMNLP    | [How Pre-trained Word Representations Capture Commonsense Physical Comparisons](https://aclanthology.org/D19-6016.pdf)                                                                       |
|   2019 | EMNLP    | [Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations](https://aclanthology.org/D19-6132.pdf)                                                                 |
|   2019 | NAACL    | [Pre-training on high-resource speech recognition improves low-resource speech-to-text translation](https://aclanthology.org/N19-1006.pdf)                                                   |
|   2019 | NAACL    | [Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data](https://aclanthology.org/N19-1014.pdf)                                           |
|   2019 | NAACL    | [Using Similarity Measures to Select Pretraining Data for NER](https://aclanthology.org/N19-1149.pdf)                                                                                        |
|   2019 | NAACL    | [An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://aclanthology.org/N19-1213.pdf)                                                             |
|   2019 | NAACL    | [Pre-trained language model representations for language generation](https://aclanthology.org/N19-1409.pdf)                                                                                  |
|   2019 | NAACL    | [Joint Learning of Pre-Trained and Random Units for Domain Adaptation in Part-of-Speech Tagging](https://aclanthology.org/N19-1416.pdf)                                                      |
|   2019 | NAACL    | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf)                                                                    |
|   2019 | NAACL    | [Leveraging Pretrained Word Embeddings for Part-of-Speech Tagging of Code Switching Data](https://aclanthology.org/W19-1410.pdf)                                                             |
|   2019 | NAACL    | [Multilingual segmentation based on neural networks and pre-trained word embeddings](https://aclanthology.org/W19-2716.pdf)                                                                  |
|   2019 | CoNLL    | [Improving Pre-Trained Multilingual Model with Vocabulary Expansion](https://aclanthology.org/K19-1030.pdf)                                                                                  |
|   2019 | CoNLL    | [Pretraining-Based Natural Language Generation for Text Summarization](https://aclanthology.org/K19-1074.pdf)                                                                                |
|   2019 | CoNLL    | [Do Massively Pretrained Language Models Make Better Storytellers?](https://aclanthology.org/K19-1079.pdf)                                                                                   |
|   2020 | ACL      | [PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable](https://aclanthology.org/2020.acl-main.9.pdf)                                                                   |
|   2020 | ACL      | [Span-ConveRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations](https://aclanthology.org/2020.acl-main.11.pdf)                                            |
|   2020 | ACL      | [Few-Shot NLG with Pre-Trained Language Model](https://aclanthology.org/2020.acl-main.18.pdf)                                                                                                |
|   2020 | ACL      | [Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders](https://aclanthology.org/2020.acl-main.23.pdf)                                                  |
|   2020 | ACL      | [SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization](https://aclanthology.org/2020.acl-main.197.pdf)                |
|   2020 | ACL      | [To Pretrain or Not to Pretrain: Examining the Benefits of Pretrainng on Resource Rich Tasks](https://aclanthology.org/2020.acl-main.200.pdf)                                                |
|   2020 | ACL      | [Integrating Multimodal Information in Large Pretrained Transformers](https://aclanthology.org/2020.acl-main.214.pdf)                                                                        |
|   2020 | ACL      | [Pretrained Transformers Improve Out-of-Distribution Robustness](https://aclanthology.org/2020.acl-main.244.pdf)                                                                             |
|   2020 | ACL      | [Span Selection Pre-training for Question Answering](https://aclanthology.org/2020.acl-main.247.pdf)                                                                                         |
|   2020 | ACL      | [Weight Poisoning Attacks on Pretrained Models](https://aclanthology.org/2020.acl-main.249.pdf)                                                                                              |
|   2020 | ACL      | [Do you have the right scissors? Tailoring Pre-trained Language Models via Monte-Carlo Methods](https://aclanthology.org/2020.acl-main.314.pdf)                                              |
|   2020 | ACL      | [Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention](https://aclanthology.org/2020.acl-main.315.pdf)                                                         |
|   2020 | ACL      | [Curriculum Pre-training for End-to-End Speech Translation](https://aclanthology.org/2020.acl-main.344.pdf)                                                                                  |
|   2020 | ACL      | [Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning](https://aclanthology.org/2020.acl-main.357.pdf)                                                             |
|   2020 | ACL      | [SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis](https://aclanthology.org/2020.acl-main.374.pdf)                                                                     |
|   2020 | ACL      | [Encoder-Decoder Models Can Benefit from Pre-trained Masked Language Models in Grammatical Error Correction](https://aclanthology.org/2020.acl-main.391.pdf)                                 |
|   2020 | ACL      | [TaPas: Weakly Supervised Table Parsing via Pre-training](https://aclanthology.org/2020.acl-main.398.pdf)                                                                                    |
|   2020 | ACL      | [DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering](https://aclanthology.org/2020.acl-main.411.pdf)                                                               |
|   2020 | ACL      | [Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings](https://aclanthology.org/2020.acl-main.431.pdf)                                                 |
|   2020 | ACL      | [Pretraining with Contrastive Sentence Objectives Improves Discourse Performance of Language Models](https://aclanthology.org/2020.acl-main.439.pdf)                                         |
|   2020 | ACL      | [Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?](https://aclanthology.org/2020.acl-main.467.pdf)                                            |
|   2020 | ACL      | [Video-Grounded Dialogues with Pretrained Generation Language Models](https://aclanthology.org/2020.acl-main.518.pdf)                                                                        |
|   2020 | ACL      | [Emerging Cross-lingual Structure in Pretrained Language Models](https://aclanthology.org/2020.acl-main.536.pdf)                                                                             |
|   2020 | ACL      | [Incorporating External Knowledge through Pre-training for Natural Language to Code Generation](https://aclanthology.org/2020.acl-main.538.pdf)                                              |
|   2020 | ACL      | [Unsupervised Domain Clusters in Pretrained Language Models](https://aclanthology.org/2020.acl-main.692.pdf)                                                                                 |
|   2020 | ACL      | [Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly](https://aclanthology.org/2020.acl-main.698.pdf)                                                |
|   2020 | ACL      | [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://aclanthology.org/2020.acl-main.703.pdf)                          |
|   2020 | ACL      | [Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://aclanthology.org/2020.acl-main.740.pdf)                                                                         |
|   2020 | ACL      | [TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data](https://aclanthology.org/2020.acl-main.745.pdf)                                                                    |
|   2020 | ACL      | [Parallel Corpus Filtering via Pre-trained Language Models](https://aclanthology.org/2020.acl-main.756.pdf)                                                                                  |
|   2020 | ACL      | [DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation](https://aclanthology.org/2020.acl-demos.30.pdf)                                                      |
|   2020 | ACL      | [Pre-training via Leveraging Assisting Languages for Neural Machine Translation](https://aclanthology.org/2020.acl-srw.37.pdf)                                                               |
|   2020 | ACL      | [Using Large Pretrained Language Models for Answering User Queries from Product Specifications](https://aclanthology.org/2020.ecnlp-1.5.pdf)                                                 |
|   2020 | ACL      | [Multi-Task Supervised Pretraining for Neural Domain Adaptation](https://aclanthology.org/2020.socialnlp-1.8.pdf)                                                                            |
|   2020 | ACL      | [Can Wikipedia Categories Improve Masked Language Model Pretraining?](https://aclanthology.org/2020.winlp-1.19.pdf)                                                                          |
|   2020 | EMNLP    | [Pre-Training Transformers as Energy-Based Cloze Models](https://aclanthology.org/2020.emnlp-main.20.pdf)                                                                                    |
|   2020 | EMNLP    | [Calibration of Pre-trained Transformers](https://aclanthology.org/2020.emnlp-main.21.pdf)                                                                                                   |
|   2020 | EMNLP    | [Cross-Thought for Sentence Encoder Pre-training](https://aclanthology.org/2020.emnlp-main.30.pdf)                                                                                           |
|   2020 | EMNLP    | [PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://aclanthology.org/2020.emnlp-main.57.pdf)                                              |
|   2020 | EMNLP    | [TOD-BERT: Pre-trained Natural Language Understanding for Task-Oriented Dialogue](https://aclanthology.org/2020.emnlp-main.66.pdf)                                                           |
|   2020 | EMNLP    | [Pre-training Entity Relation Encoder with Intra-span and Inter-span Information](https://aclanthology.org/2020.emnlp-main.132.pdf)                                                          |
|   2020 | EMNLP    | [Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!](https://aclanthology.org/2020.emnlp-main.135.pdf)                                         |
|   2020 | EMNLP    | [HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training](https://aclanthology.org/2020.emnlp-main.161.pdf)                                                           |
|   2020 | EMNLP    | [Masking as an Efficient Alternative to Finetuning for Pretrained Language Models](https://aclanthology.org/2020.emnlp-main.174.pdf)                                                         |
|   2020 | EMNLP    | [Improving AMR Parsing with Sequence-to-Sequence Pre-training](https://aclanthology.org/2020.emnlp-main.196.pdf)                                                                             |
|   2020 | EMNLP    | [CSP:Code-Switching Pre-training for Neural Machine Translation](https://aclanthology.org/2020.emnlp-main.208.pdf)                                                                           |
|   2020 | EMNLP    | [Pre-training Multilingual Neural Machine Translation by Leveraging Alignment Information](https://aclanthology.org/2020.emnlp-main.210.pdf)                                                 |
|   2020 | EMNLP    | [Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](https://aclanthology.org/2020.emnlp-main.214.pdf)                                               |
|   2020 | EMNLP    | [Knowledge-Grounded Dialogue Generation with Pre-trained Language Models](https://aclanthology.org/2020.emnlp-main.272.pdf)                                                                  |
|   2020 | EMNLP    | [Modeling Content Importance for Summarization with Pre-trained Language Models](https://aclanthology.org/2020.emnlp-main.293.pdf)                                                           |
|   2020 | EMNLP    | [Pre-training for Abstractive Document Summarization by Reinstating Source Text](https://aclanthology.org/2020.emnlp-main.297.pdf)                                                           |
|   2020 | EMNLP    | [DagoBERT: Generating Derivational Morphology with a Pretrained Language Model](https://aclanthology.org/2020.emnlp-main.316.pdf)                                                            |
|   2020 | EMNLP    | [Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space](https://aclanthology.org/2020.emnlp-main.378.pdf)                                                                 |
|   2020 | EMNLP    | [An Empirical Study of Pre-trained Transformers for Arabic Information Extraction](https://aclanthology.org/2020.emnlp-main.382.pdf)                                                         |
|   2020 | EMNLP    | [TNT: Text Normalization based Pre-training of Transformers for Content Moderation](https://aclanthology.org/2020.emnlp-main.383.pdf)                                                        |
|   2020 | EMNLP    | [An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training](https://aclanthology.org/2020.emnlp-main.394.pdf)                                                    |
|   2020 | EMNLP    | [Analyzing Individual Neurons in Pre-trained Language Models](https://aclanthology.org/2020.emnlp-main.395.pdf)                                                                              |
|   2020 | EMNLP    | [Analyzing Redundancy in Pretrained Transformer Models](https://aclanthology.org/2020.emnlp-main.398.pdf)                                                                                    |
|   2020 | EMNLP    | [Multi-Stage Pre-training for Low-Resource Domain Adaptation](https://aclanthology.org/2020.emnlp-main.440.pdf)                                                                              |
|   2020 | EMNLP    | [ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention](https://aclanthology.org/2020.emnlp-main.441.pdf)                                  |
|   2020 | EMNLP    | [X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models](https://aclanthology.org/2020.emnlp-main.479.pdf)                                                        |
|   2020 | EMNLP    | [XGLUE: A New Benchmark Datasetfor Cross-lingual Pre-training, Understanding and Generation](https://aclanthology.org/2020.emnlp-main.484.pdf)                                               |
|   2020 | EMNLP    | [Coarse-to-Fine Pre-training for Named Entity Recognition](https://aclanthology.org/2020.emnlp-main.514.pdf)                                                                                 |
|   2020 | EMNLP    | [Entity Enhanced BERT Pre-training for Chinese NER](https://aclanthology.org/2020.emnlp-main.518.pdf)                                                                                        |
|   2020 | EMNLP    | [Multi-Stage Pre-training for Automated Chinese Essay Scoring](https://aclanthology.org/2020.emnlp-main.546.pdf)                                                                             |
|   2020 | EMNLP    | [Pretrained Language Model Embryology: The Birth of ALBERT](https://aclanthology.org/2020.emnlp-main.553.pdf)                                                                                |
|   2020 | EMNLP    | [What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding](https://aclanthology.org/2020.emnlp-main.555.pdf)                                  |
|   2020 | EMNLP    | [“You are grounded!”: Latent Name Artifacts in Pre-trained Language Models](https://aclanthology.org/2020.emnlp-main.556.pdf)                                                                |
|   2020 | EMNLP    | [Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models](https://aclanthology.org/2020.emnlp-main.557.pdf)                                |
|   2020 | EMNLP    | [Train No Evil: Selective Masking for Task-Guided Pre-Training](https://aclanthology.org/2020.emnlp-main.566.pdf)                                                                            |
|   2020 | EMNLP    | [Probing Pretrained Language Models for Lexical Semantics](https://aclanthology.org/2020.emnlp-main.586.pdf)                                                                                 |
|   2020 | EMNLP    | [A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?](https://aclanthology.org/2020.emnlp-main.592.pdf)                                |
|   2020 | EMNLP    | [Feature Adaptation of Pre-Trained Language Models across Languages and Domains with Robust Self-Training](https://aclanthology.org/2020.emnlp-main.599.pdf)                                 |
|   2020 | EMNLP    | [On the importance of pre-training data volume for compact language models](https://aclanthology.org/2020.emnlp-main.632.pdf)                                                                |
|   2020 | EMNLP    | [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://aclanthology.org/2020.emnlp-main.634.pdf)                                                       |
|   2020 | EMNLP    | [Effectively pretraining a speech translation decoder with Machine Translation data](https://aclanthology.org/2020.emnlp-main.644.pdf)                                                       |
|   2020 | EMNLP    | [Pre-training Mention Representations in Coreference Models](https://aclanthology.org/2020.emnlp-main.687.pdf)                                                                               |
|   2020 | EMNLP    | [KGPT: Knowledge-Grounded Pre-Training for Data-to-Text Generation](https://aclanthology.org/2020.emnlp-main.697.pdf)                                                                        |
|   2020 | EMNLP    | [POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training](https://aclanthology.org/2020.emnlp-main.698.pdf)                                             |
|   2020 | EMNLP    | [PALM: Pre-training an Autoencoding&Autoregressive Language Model for Context-conditioned Generation](https://aclanthology.org/2020.emnlp-main.700.pdf)                                      |
|   2020 | EMNLP    | [On the Sentence Embeddings from Pre-trained Language Models](https://aclanthology.org/2020.emnlp-main.733.pdf)                                                                              |
|   2020 | EMNLP    | [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2.pdf)                                                                                 |
|   2020 | EMNLP    | [On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers](https://aclanthology.org/2020.blackboxnlp-1.7.pdf)                    |
|   2020 | EMNLP    | [BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining](https://aclanthology.org/2020.clinicalnlp-1.3.pdf)                                                                       |
|   2020 | EMNLP    | [Incorporating Risk Factor Embeddings in Pre-trained Transformers Improves Sentiment Prediction in Psychiatric Discharge Summaries](https://aclanthology.org/2020.clinicalnlp-1.4.pdf)       |
|   2020 | EMNLP    | [MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining](https://aclanthology.org/2020.clinicalnlp-1.15.pdf)                                      |
|   2020 | EMNLP    | [Pretrained Language Models for Biomedical and Clinical Tasks: Understanding and Extending the State-of-the-Art](https://aclanthology.org/2020.clinicalnlp-1.17.pdf)                         |
|   2020 | EMNLP    | [Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics](https://aclanthology.org/2020.clinicalnlp-1.26.pdf)           |
|   2020 | EMNLP    | [Common Sense or World Knowledge? Investigating Adapter-Based Knowledge Injection into Pretrained Transformers](https://aclanthology.org/2020.deelio-1.5.pdf)                                |
|   2020 | EMNLP    | [Incorporating Commonsense Knowledge Graph in Pretrained Models for Social Commonsense Tasks](https://aclanthology.org/2020.deelio-1.9.pdf)                                                  |
|   2020 | EMNLP    | [A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining](https://aclanthology.org/2020.findings-emnlp.19.pdf)                                            |
|   2020 | EMNLP    | [Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.25.pdf)                                                       |
|   2020 | EMNLP    | [Understanding tables with intermediate pre-training](https://aclanthology.org/2020.findings-emnlp.27.pdf)                                                                                   |
|   2020 | EMNLP    | [Revisiting Pre-Trained Models for Chinese Natural Language Processing](https://aclanthology.org/2020.findings-emnlp.58.pdf)                                                                 |
|   2020 | EMNLP    | [Document Ranking with a Pretrained Sequence-to-Sequence Model](https://aclanthology.org/2020.findings-emnlp.63.pdf)                                                                         |
|   2020 | EMNLP    | [Pretrained Language Models for Dialogue Generation with Multiple Input Sources](https://aclanthology.org/2020.findings-emnlp.81.pdf)                                                        |
|   2020 | EMNLP    | [PhoBERT: Pre-trained language models for Vietnamese](https://aclanthology.org/2020.findings-emnlp.92.pdf)                                                                                   |
|   2020 | EMNLP    | [Investigating Transferability in Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.125.pdf)                                                                          |
|   2020 | EMNLP    | [exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources](https://aclanthology.org/2020.findings-emnlp.129.pdf)                            |
|   2020 | EMNLP    | [Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA](https://aclanthology.org/2020.findings-emnlp.134.pdf)                          |
|   2020 | EMNLP    | [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://aclanthology.org/2020.findings-emnlp.139.pdf)                                                                  |
|   2020 | EMNLP    | [StyleDGPT: Stylized Response Generation with Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.140.pdf)                                                             |
|   2020 | EMNLP    | [Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of Regression and Ranking](https://aclanthology.org/2020.findings-emnlp.141.pdf) |
|   2020 | EMNLP    | [On the Language Neutrality of Pre-trained Multilingual Representations](https://aclanthology.org/2020.findings-emnlp.150.pdf)                                                               |
|   2020 | EMNLP    | [Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media](https://aclanthology.org/2020.findings-emnlp.151.pdf)                                       |
|   2020 | EMNLP    | [An Empirical Exploration of Local Ordering Pre-training for Structured Prediction](https://aclanthology.org/2020.findings-emnlp.160.pdf)                                                    |
|   2020 | EMNLP    | [Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers](https://aclanthology.org/2020.findings-emnlp.161.pdf)                                                      |
|   2020 | EMNLP    | [TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising](https://aclanthology.org/2020.findings-emnlp.168.pdf)                                                 |
|   2020 | EMNLP    | [PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision](https://aclanthology.org/2020.findings-emnlp.174.pdf)                                                     |
|   2020 | EMNLP    | [Multi-pretraining for Large-scale Text Classification](https://aclanthology.org/2020.findings-emnlp.185.pdf)                                                                                |
|   2020 | EMNLP    | [A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning](https://aclanthology.org/2020.findings-emnlp.206.pdf)                           |
|   2020 | EMNLP    | [BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.207.pdf)                                                 |
|   2020 | EMNLP    | [ProphetNet: Predicting Future N-gram for Sequence-to-SequencePre-training](https://aclanthology.org/2020.findings-emnlp.217.pdf)                                                            |
|   2020 | EMNLP    | [On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers](https://aclanthology.org/2020.findings-emnlp.227.pdf)                 |
|   2020 | EMNLP    | [Hierarchical Pre-training for Sequence Labelling in Spoken Dialog](https://aclanthology.org/2020.findings-emnlp.239.pdf)                                                                    |
|   2020 | EMNLP    | [Can Pre-training help VQA with Lexical Variations?](https://aclanthology.org/2020.findings-emnlp.257.pdf)                                                                                   |
|   2020 | EMNLP    | [General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference](https://aclanthology.org/2020.findings-emnlp.271.pdf)                                              |
|   2020 | EMNLP    | [Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning](https://aclanthology.org/2020.findings-emnlp.285.pdf)                                   |
|   2020 | EMNLP    | [Improving Event Duration Prediction via Time-aware Pre-training](https://aclanthology.org/2020.findings-emnlp.302.pdf)                                                                      |
|   2020 | EMNLP    | [BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA](https://aclanthology.org/2020.findings-emnlp.307.pdf)                                                  |
|   2020 | EMNLP    | [Context Analysis for Pre-trained Masked Language Models](https://aclanthology.org/2020.findings-emnlp.338.pdf)                                                                              |
|   2020 | EMNLP    | [Large Product Key Memory for Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.362.pdf)                                                                              |
|   2020 | EMNLP    | [How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://aclanthology.org/2020.findings-emnlp.394.pdf)                                                        |
|   2020 | EMNLP    | [On the Branching Bias of Syntax Extracted from Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.401.pdf)                                                           |
|   2020 | EMNLP    | [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://aclanthology.org/2020.findings-emnlp.414.pdf)                                                                      |
|   2020 | EMNLP    | [ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations](https://aclanthology.org/2020.findings-emnlp.425.pdf)                                                            |
|   2020 | EMNLP    | [Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER](https://aclanthology.org/2020.findings-emnlp.429.pdf)                                                 |
|   2020 | EMNLP    | [IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages](https://aclanthology.org/2020.findings-emnlp.445.pdf)          |
|   2020 | EMNLP    | [Do Transformers Dream of Inference, or Can Pretrained Generative Models Learn Implicit Inferential Rules?](https://aclanthology.org/2020.insights-1.12.pdf)                                 |
|   2020 | EMNLP    | [Identifying Personal Experience Tweets of Medication Effects Using Pre-trained RoBERTa Language Model and Its Updating](https://aclanthology.org/2020.louhi-1.14.pdf)                       |
|   2020 | EMNLP    | [Building a Bridge: A Method for Image-Text Sarcasm Detection Without Pretraining on Image-Text Data](https://aclanthology.org/2020.nlpbt-1.3.pdf)                                           |
|   2020 | EMNLP    | [Differentially Private Language Models Benefit from Public Pre-training](https://aclanthology.org/2020.privatenlp-1.5.pdf)                                                                  |
|   2020 | EMNLP    | [On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining](https://aclanthology.org/2020.sdp-1.12.pdf)                          |
|   2020 | EMNLP    | [Using Pre-Trained Transformer for Better Lay Summarization](https://aclanthology.org/2020.sdp-1.38.pdf)                                                                                     |
|   2020 | EMNLP    | [A Two-stage Model for Slot Filling in Low-resource Settings: Domain-agnostic Non-slot Reduction and Pretrained Contextual Embeddings](https://aclanthology.org/2020.sustainlp-1.10.pdf)     |
|   2020 | EMNLP    | [Machine Translation for English–Inuktitut with Segmentation, Data Acquisition and Pre-Training](https://aclanthology.org/2020.wmt-1.29.pdf)                                                 |
|   2020 | EMNLP    | [Language Models not just for Pre-training: Fast Online Neural Noisy Channel Modeling](https://aclanthology.org/2020.wmt-1.69.pdf)                                                           |
|   2020 | EMNLP    | [Pretrained Language Models and Backtranslation for English-Basque Biomedical Neural Machine Translation](https://aclanthology.org/2020.wmt-1.89.pdf)                                        |
|   2020 | EMNLP    | [Extended Study on Using Pretrained Language Models and YiSi-1 for Machine Translation Evaluation](https://aclanthology.org/2020.wmt-1.99.pdf)                                               |
|   2020 | EMNLP    | [Improving Parallel Data Identification using Iteratively Refined Sentence Alignments and Bilingual Mappings of Pre-trained Language Models](https://aclanthology.org/2020.wmt-1.110.pdf)    |
|   2020 | COLING   | [CharBERT: Character-aware Pre-trained Language Model](https://aclanthology.org/2020.coling-main.4.pdf)                                                                                      |
|   2020 | COLING   | [Understanding Pre-trained BERT for Aspect-based Sentiment Analysis](https://aclanthology.org/2020.coling-main.21.pdf)                                                                       |
|   2020 | COLING   | [SentiX: A Sentiment-Aware Pre-Trained Model for Cross-Domain Sentiment Analysis](https://aclanthology.org/2020.coling-main.49.pdf)                                                          |
|   2020 | COLING   | [BioMedBERT: A Pre-trained Biomedical Language Model for QA and IR](https://aclanthology.org/2020.coling-main.59.pdf)                                                                        |
|   2020 | COLING   | [IndoLEM and IndoBERT: A Benchmark Dataset and Pre-trained Language Model for Indonesian NLP](https://aclanthology.org/2020.coling-main.66.pdf)                                              |
|   2020 | COLING   | [Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity](https://aclanthology.org/2020.coling-main.118.pdf)                                                         |
|   2020 | COLING   | [Pre-trained Language Model Based Active Learning for Sentence Matching](https://aclanthology.org/2020.coling-main.130.pdf)                                                                  |
|   2020 | COLING   | [Autoencoding Improves Pre-trained Word Embeddings](https://aclanthology.org/2020.coling-main.149.pdf)                                                                                       |
|   2020 | COLING   | [Multi-Task Learning for Knowledge Graph Completion with Pre-trained Language Models](https://aclanthology.org/2020.coling-main.153.pdf)                                                     |
|   2020 | COLING   | [RIVA: A Pre-trained Tweet Multimodal Model Based on Text-image Relation for Multimodal NER](https://aclanthology.org/2020.coling-main.168.pdf)                                              |
|   2020 | COLING   | [Multi-level Alignment Pretraining for Multi-lingual Semantic Parsing](https://aclanthology.org/2020.coling-main.289.pdf)                                                                    |
|   2020 | COLING   | [An Empirical Study of the Downstream Reliability of Pre-Trained Word Embeddings](https://aclanthology.org/2020.coling-main.299.pdf)                                                         |
|   2020 | COLING   | [Designing Templates for Eliciting Commonsense Knowledge from Pretrained Sequence-to-Sequence Models](https://aclanthology.org/2020.coling-main.307.pdf)                                     |
|   2020 | COLING   | [Evaluating Pretrained Transformer-based Models on the Task of Fine-Grained Named Entity Recognition](https://aclanthology.org/2020.coling-main.334.pdf)                                     |
|   2020 | COLING   | [Unleashing the Power of Neural Discourse Parsers - A Context and Structure Aware Approach Using Large Scale Pretraining](https://aclanthology.org/2020.coling-main.337.pdf)                 |
|   2020 | COLING   | [Semi-supervised URL Segmentation with Recurrent Neural Networks Pre-trained on Knowledge Graph Entities](https://aclanthology.org/2020.coling-main.411.pdf)                                 |
|   2020 | COLING   | [Emergent Communication Pretraining for Few-Shot Machine Translation](https://aclanthology.org/2020.coling-main.416.pdf)                                                                     |
|   2020 | COLING   | [Scientific Keyphrase Identification and Classification by Pre-Trained Language Models Intermediate Task Transfer Learning](https://aclanthology.org/2020.coling-main.472.pdf)               |
|   2020 | COLING   | [Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification](https://aclanthology.org/2020.coling-main.481.pdf)                                |
|   2020 | COLING   | [Detecting Non-literal Translations by Fine-tuning Cross-lingual Pre-trained Language Models](https://aclanthology.org/2020.coling-main.522.pdf)                                             |
|   2020 | COLING   | [NILC at SR’20: Exploring Pre-Trained Models in Surface Realisation](https://aclanthology.org/2020.msr-1.6.pdf)                                                                              |
|   2020 | COLING   | [Surface Realization Using Pretrained Language Models](https://aclanthology.org/2020.msr-1.7.pdf)                                                                                            |
|   2020 | COLING   | [Incorporating Count-Based Features into Pre-Trained Models for Improved Stance Detection](https://aclanthology.org/2020.nlp4if-1.3.pdf)                                                     |
|   2020 | COLING   | [Contextual Augmentation of Pretrained Language Models for Emotion Recognition in Conversations](https://aclanthology.org/2020.peoples-1.7.pdf)                                              |
|   2020 | COLING   | [Identification of Medication Tweets Using Domain-specific Pre-trained Language Models](https://aclanthology.org/2020.smm4h-1.22.pdf)                                                        |
|   2020 | COLING   | [Automatic Classification of Tweets Mentioning a Medication Using Pre-trained Sentence Encoders](https://aclanthology.org/2020.smm4h-1.27.pdf)                                               |
|   2020 | AACL     | [Can Monolingual Pretrained Models Help Cross-Lingual Classification?](https://aclanthology.org/2020.aacl-main.2.pdf)                                                                        |
|   2020 | AACL     | [Chinese Grammatical Correction Using BERT-based Pre-trained Model](https://aclanthology.org/2020.aacl-main.20.pdf)                                                                          |
|   2020 | AACL     | [UnihanLM: Coarse-to-Fine Chinese-Japanese Language Model Pretraining with the Unihan Database](https://aclanthology.org/2020.aacl-main.24.pdf)                                              |
|   2020 | AACL     | [Multimodal Pretraining for Dense Video Captioning](https://aclanthology.org/2020.aacl-main.48.pdf)                                                                                          |
|   2020 | AACL     | [Mixed-Lingual Pre-training for Cross-lingual Summarization](https://aclanthology.org/2020.aacl-main.53.pdf)                                                                                 |
|   2020 | AACL     | [DAPPER: Learning Domain-Adapted Persona Representation Using Pretrained BERT and External Memory](https://aclanthology.org/2020.aacl-main.65.pdf)                                           |
|   2020 | AACL     | [Stronger Baselines for Grammatical Error Correction Using a Pretrained Encoder-Decoder Model](https://aclanthology.org/2020.aacl-main.83.pdf)                                               |
|   2020 | AACL     | [Compressing Pre-trained Language Models by Matrix Decomposition](https://aclanthology.org/2020.aacl-main.88.pdf)                                                                            |
|   2020 | AACL     | [Data Augmentation using Pre-trained Transformer Models](https://aclanthology.org/2020.lifelongnlp-1.3.pdf)                                                                                  |
|   2020 | Findings | [A Hierarchical Network for Abstractive Meeting Summarization with Cross-Domain Pretraining](https://aclanthology.org/2020.findings-emnlp.19.pdf)                                            |
|   2020 | Findings | [Pretrain-KGE: Learning Knowledge Representation from Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.25.pdf)                                                       |
|   2020 | Findings | [Understanding tables with intermediate pre-training](https://aclanthology.org/2020.findings-emnlp.27.pdf)                                                                                   |
|   2020 | Findings | [Revisiting Pre-Trained Models for Chinese Natural Language Processing](https://aclanthology.org/2020.findings-emnlp.58.pdf)                                                                 |
|   2020 | Findings | [Document Ranking with a Pretrained Sequence-to-Sequence Model](https://aclanthology.org/2020.findings-emnlp.63.pdf)                                                                         |
|   2020 | Findings | [Pretrained Language Models for Dialogue Generation with Multiple Input Sources](https://aclanthology.org/2020.findings-emnlp.81.pdf)                                                        |
|   2020 | Findings | [PhoBERT: Pre-trained language models for Vietnamese](https://aclanthology.org/2020.findings-emnlp.92.pdf)                                                                                   |
|   2020 | Findings | [Investigating Transferability in Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.125.pdf)                                                                          |
|   2020 | Findings | [exBERT: Extending Pre-trained Models with Domain-specific Vocabulary Under Constrained Training Resources](https://aclanthology.org/2020.findings-emnlp.129.pdf)                            |
|   2020 | Findings | [Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA](https://aclanthology.org/2020.findings-emnlp.134.pdf)                          |
|   2020 | Findings | [CodeBERT: A Pre-Trained Model for Programming and Natural Languages](https://aclanthology.org/2020.findings-emnlp.139.pdf)                                                                  |
|   2020 | Findings | [StyleDGPT: Stylized Response Generation with Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.140.pdf)                                                             |
|   2020 | Findings | [Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of Regression and Ranking](https://aclanthology.org/2020.findings-emnlp.141.pdf) |
|   2020 | Findings | [On the Language Neutrality of Pre-trained Multilingual Representations](https://aclanthology.org/2020.findings-emnlp.150.pdf)                                                               |
|   2020 | Findings | [Cost-effective Selection of Pretraining Data: A Case Study of Pretraining BERT on Social Media](https://aclanthology.org/2020.findings-emnlp.151.pdf)                                       |
|   2020 | Findings | [An Empirical Exploration of Local Ordering Pre-training for Structured Prediction](https://aclanthology.org/2020.findings-emnlp.160.pdf)                                                    |
|   2020 | Findings | [Unsupervised Extractive Summarization by Pre-training Hierarchical Transformers](https://aclanthology.org/2020.findings-emnlp.161.pdf)                                                      |
|   2020 | Findings | [TED: A Pretrained Unsupervised Summarization Model with Theme Modeling and Denoising](https://aclanthology.org/2020.findings-emnlp.168.pdf)                                                 |
|   2020 | Findings | [PTUM: Pre-training User Model from Unlabeled User Behaviors via Self-supervision](https://aclanthology.org/2020.findings-emnlp.174.pdf)                                                     |
|   2020 | Findings | [Multi-pretraining for Large-scale Text Classification](https://aclanthology.org/2020.findings-emnlp.185.pdf)                                                                                |
|   2020 | Findings | [A Semi-supervised Approach to Generate the Code-Mixed Text using Pre-trained Encoder and Transfer Learning](https://aclanthology.org/2020.findings-emnlp.206.pdf)                           |
|   2020 | Findings | [BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.207.pdf)                                                 |
|   2020 | Findings | [ProphetNet: Predicting Future N-gram for Sequence-to-SequencePre-training](https://aclanthology.org/2020.findings-emnlp.217.pdf)                                                            |
|   2020 | Findings | [On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers](https://aclanthology.org/2020.findings-emnlp.227.pdf)                 |
|   2020 | Findings | [Hierarchical Pre-training for Sequence Labelling in Spoken Dialog](https://aclanthology.org/2020.findings-emnlp.239.pdf)                                                                    |
|   2020 | Findings | [Can Pre-training help VQA with Lexical Variations?](https://aclanthology.org/2020.findings-emnlp.257.pdf)                                                                                   |
|   2020 | Findings | [General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference](https://aclanthology.org/2020.findings-emnlp.271.pdf)                                              |
|   2020 | Findings | [Integrating Task Specific Information into Pretrained Language Models for Low Resource Fine Tuning](https://aclanthology.org/2020.findings-emnlp.285.pdf)                                   |
|   2020 | Findings | [Improving Event Duration Prediction via Time-aware Pre-training](https://aclanthology.org/2020.findings-emnlp.302.pdf)                                                                      |
|   2020 | Findings | [BERT-kNN: Adding a kNN Search Component to Pretrained Language Models for Better QA](https://aclanthology.org/2020.findings-emnlp.307.pdf)                                                  |
|   2020 | Findings | [Context Analysis for Pre-trained Masked Language Models](https://aclanthology.org/2020.findings-emnlp.338.pdf)                                                                              |
|   2020 | Findings | [Large Product Key Memory for Pretrained Language Models](https://aclanthology.org/2020.findings-emnlp.362.pdf)                                                                              |
|   2020 | Findings | [How Effective is Task-Agnostic Data Augmentation for Pretrained Transformers?](https://aclanthology.org/2020.findings-emnlp.394.pdf)                                                        |
|   2020 | Findings | [On the Branching Bias of Syntax Extracted from Pre-trained Language Models](https://aclanthology.org/2020.findings-emnlp.401.pdf)                                                           |
|   2020 | Findings | [Byte Pair Encoding is Suboptimal for Language Model Pretraining](https://aclanthology.org/2020.findings-emnlp.414.pdf)                                                                      |
|   2020 | Findings | [ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations](https://aclanthology.org/2020.findings-emnlp.425.pdf)                                                            |
|   2020 | Findings | [Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER](https://aclanthology.org/2020.findings-emnlp.429.pdf)                                                 |
|   2020 | Findings | [IndicNLPSuite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for Indian Languages](https://aclanthology.org/2020.findings-emnlp.445.pdf)          |
|   2020 | CoNLL    | [Are Pretrained Language Models Symbolic Reasoners over Knowledge?](https://aclanthology.org/2020.conll-1.45.pdf)                                                                            |
|   2020 | TACL     | [SpanBERT: Improving Pre-training by Representing and Predicting Spans](https://aclanthology.org/2020.tacl-1.5.pdf)                                                                          |
|   2020 | TACL     | [A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation](https://aclanthology.org/2020.tacl-1.7.pdf)                                                                        |
|   2020 | TACL     | [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://aclanthology.org/2020.tacl-1.18.pdf)                                                                              |
|   2020 | TACL     | [PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized Embedding Models](https://aclanthology.org/2020.tacl-1.33.pdf)                                                      |
|   2020 | TACL     | [An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models](https://aclanthology.org/2020.tacl-1.40.pdf)                                                   |
|   2020 | TACL     | [Multilingual Denoising Pre-training for Neural Machine Translation](https://aclanthology.org/2020.tacl-1.47.pdf)                                                                            |
|   2020 | TACL     | [oLMpics-On What Language Model Pre-training Captures](https://aclanthology.org/2020.tacl-1.48.pdf)                                                                                          |
|   2020 | TACL     | [Syntactic Structure Distillation Pretraining for Bidirectional Encoders](https://aclanthology.org/2020.tacl-1.50.pdf)                                                                       |
|   2020 | TACL     | [Improving Dialog Evaluation with a Multi-reference Adversarial Dataset and Large Scale Pretraining](https://aclanthology.org/2020.tacl-1.52.pdf)                                            |
|   2021 | ACL      | [E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning](https://aclanthology.org/2021.acl-long.42.pdf)                                                                |
|   2021 | ACL      | [Multilingual Speech Translation from Efficient Finetuning of Pretrained Models](https://aclanthology.org/2021.acl-long.68.pdf)                                                              |
|   2021 | ACL      | [XLPT-AMR: Cross-Lingual Pre-Training via Multi-Task Learning for Zero-Shot AMR Parsing and Text Generation](https://aclanthology.org/2021.acl-long.73.pdf)                                  |
|   2021 | ACL      | [Unsupervised Out-of-Domain Detection via Pre-trained Transformers](https://aclanthology.org/2021.acl-long.85.pdf)                                                                           |
|   2021 | ACL      | [When Do You Need Billions of Words of Pretraining Data?](https://aclanthology.org/2021.acl-long.90.pdf)                                                                                     |
|   2021 | ACL      | [Diverse Pretrained Context Encodings Improve Document Translation](https://aclanthology.org/2021.acl-long.104.pdf)                                                                          |
|   2021 | ACL      | [ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information](https://aclanthology.org/2021.acl-long.161.pdf)                                                                  |
|   2021 | ACL      | [BERTAC: Enhancing Transformer-based Language Models with Adversarially Pretrained Convolutional Neural Networks](https://aclanthology.org/2021.acl-long.164.pdf)                            |
|   2021 | ACL      | [On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://aclanthology.org/2021.acl-long.172.pdf)                                                      |
|   2021 | ACL      | [An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Models](https://aclanthology.org/2021.acl-long.178.pdf)                                              |
|   2021 | ACL      | [Multi-stage Pre-training over Simplified Multimodal Pre-training Models](https://aclanthology.org/2021.acl-long.199.pdf)                                                                    |
|   2021 | ACL      | [LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding](https://aclanthology.org/2021.acl-long.201.pdf)                                                              |
|   2021 | ACL      | [Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders](https://aclanthology.org/2021.acl-long.204.pdf)                                 |
|   2021 | ACL      | [Breaking the Corpus Bottleneck for Context-Aware Neural Machine Translation with Cross-Task Pre-training](https://aclanthology.org/2021.acl-long.222.pdf)                                   |
|   2021 | ACL      | [PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction](https://aclanthology.org/2021.acl-long.233.pdf)                                                              |
|   2021 | ACL      | [Few-Shot Question Answering by Pretraining Span Selection](https://aclanthology.org/2021.acl-long.239.pdf)                                                                                  |
|   2021 | ACL      | [Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?](https://aclanthology.org/2021.acl-long.251.pdf)                                                    |
|   2021 | ACL      | [Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation](https://aclanthology.org/2021.acl-long.259.pdf)                                          |
|   2021 | ACL      | [ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning](https://aclanthology.org/2021.acl-long.260.pdf)                                |
|   2021 | ACL      | [Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment](https://aclanthology.org/2021.acl-long.265.pdf)                                                         |
|   2021 | ACL      | [BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?](https://aclanthology.org/2021.acl-long.280.pdf)                                                  |
|   2021 | ACL      | [MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding](https://aclanthology.org/2021.acl-long.285.pdf)                                                          |
|   2021 | ACL      | [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295.pdf)                                                                                |
|   2021 | ACL      | [VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation](https://aclanthology.org/2021.acl-long.308.pdf)                                           |
|   2021 | ACL      | [Probing Toxic Content in Large Pre-Trained Language Models](https://aclanthology.org/2021.acl-long.329.pdf)                                                                                 |
|   2021 | ACL      | [Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models](https://aclanthology.org/2021.acl-long.333.pdf)                                         |
|   2021 | ACL      | [Are Pretrained Convolutions Better than Pretrained Transformers?](https://aclanthology.org/2021.acl-long.335.pdf)                                                                           |
|   2021 | ACL      | [A Pre-training Strategy for Zero-Resource Response Selection in Knowledge-Grounded Conversations](https://aclanthology.org/2021.acl-long.343.pdf)                                           |
|   2021 | ACL      | [SemFace: Pre-training Encoder and Decoder with a Semantic Interface for Neural Machine Translation](https://aclanthology.org/2021.acl-long.348.pdf)                                         |
|   2021 | ACL      | [How to Adapt Your Pretrained Multilingual Model to 1600 Languages](https://aclanthology.org/2021.acl-long.351.pdf)                                                                          |
|   2021 | ACL      | [Pre-training Universal Language Representation](https://aclanthology.org/2021.acl-long.398.pdf)                                                                                             |
|   2021 | ACL      | [Structural Pre-training for Dialogue Comprehension](https://aclanthology.org/2021.acl-long.399.pdf)                                                                                         |
|   2021 | ACL      | [AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models](https://aclanthology.org/2021.acl-long.400.pdf)                                             |
|   2021 | ACL      | [LexFit: Lexical Fine-Tuning of Pretrained Language Models](https://aclanthology.org/2021.acl-long.410.pdf)                                                                                  |
|   2021 | ACL      | [StereoSet: Measuring stereotypical bias in pretrained language models](https://aclanthology.org/2021.acl-long.416.pdf)                                                                      |
|   2021 | ACL      | [Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators](https://aclanthology.org/2021.acl-long.418.pdf)                              |
|   2021 | ACL      | [Syntax-Enhanced Pre-trained Model](https://aclanthology.org/2021.acl-long.420.pdf)                                                                                                          |
|   2021 | ACL      | [SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining](https://aclanthology.org/2021.acl-long.457.pdf)                                |
|   2021 | ACL      | [Bridging Subword Gaps in Pretrain-Finetune Paradigm for Natural Language Generation](https://aclanthology.org/2021.acl-long.468.pdf)                                                        |
|   2021 | ACL      | [TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models](https://aclanthology.org/2021.acl-long.469.pdf)                                    |
|   2021 | ACL      | [CLEVE: Contrastive Pre-training for Event Extraction](https://aclanthology.org/2021.acl-long.491.pdf)                                                                                       |
|   2021 | ACL      | [StructuralLM: Structural Pre-training for Form Understanding](https://aclanthology.org/2021.acl-long.493.pdf)                                                                               |
|   2021 | ACL      | [Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization](https://aclanthology.org/2021.acl-long.510.pdf)                                           |
|   2021 | ACL      | [eMLM: A New Pre-training Objective for Emotion Related Tasks](https://aclanthology.org/2021.acl-short.38.pdf)                                                                               |
|   2021 | ACL      | [PRAL: A Tailored Pre-Training Model for Task-Oriented Dialog Generation](https://aclanthology.org/2021.acl-short.40.pdf)                                                                    |
|   2021 | ACL      | [Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer](https://aclanthology.org/2021.acl-short.62.pdf)                                                             |
|   2021 | ACL      | [QA-Driven Zero-shot Slot Filling with Weak Supervision Pretraining](https://aclanthology.org/2021.acl-short.83.pdf)                                                                         |
|   2021 | ACL      | [Domain-Adaptive Pretraining Methods for Dialogue Understanding](https://aclanthology.org/2021.acl-short.84.pdf)                                                                             |
|   2021 | ACL      | [nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?](https://aclanthology.org/2021.acl-short.87.pdf)                                            |
|   2021 | ACL      | [Pre-training is a Hot Topic: Contextualized Document Embeddings Improve Topic Coherence](https://aclanthology.org/2021.acl-short.96.pdf)                                                    |
|   2021 | ACL      | [Robust Transfer Learning with Pretrained Language Models through Adapters](https://aclanthology.org/2021.acl-short.108.pdf)                                                                 |
|   2021 | ACL      | [Long Document Summarization in a Low Resource Setting using Pretrained Language Models](https://aclanthology.org/2021.acl-srw.7.pdf)                                                        |
|   2021 | ACL      | [Modeling Text using the Continuous Space Topic Model with Pre-Trained Word Embeddings](https://aclanthology.org/2021.acl-srw.15.pdf)                                                        |
|   2021 | ACL      | [MVP-BERT: Multi-Vocab Pre-training for Chinese BERT](https://aclanthology.org/2021.acl-srw.27.pdf)                                                                                          |
|   2021 | ACL      | [ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation](https://aclanthology.org/2021.acl-demo.28.pdf)                              |
|   2021 | ACL      | [Many-to-English Machine Translation Tools, Data, and Pretrained Models](https://aclanthology.org/2021.acl-demo.37.pdf)                                                                      |
|   2021 | ACL      | [Pre-training Methods for Neural Machine Translation](https://aclanthology.org/2021.acl-tutorials.4.pdf)                                                                                     |
|   2021 | ACL      | [Document-Grounded Goal-Oriented Dialogue Systems on Pre-Trained Language Model with Diverse Input Representation](https://aclanthology.org/2021.dialdoc-1.12.pdf)                           |
|   2021 | ACL      | [Team JARS: DialDoc Subtask 1 - Improved Knowledge Identification with Supervised Out-of-Domain Pretraining](https://aclanthology.org/2021.dialdoc-1.13.pdf)                                 |
|   2021 | ACL      | [CommitBERT: Commit Message Generation Using Pre-Trained Programming Language Model](https://aclanthology.org/2021.nlp4prog-1.3.pdf)                                                         |
|   2021 | ACL      | [Preserving Cross-Linguality of Pre-trained Models via Continual Learning](https://aclanthology.org/2021.repl4nlp-1.8.pdf)                                                                   |
|   2021 | ACL      | [Revisiting Pretraining with Adapters](https://aclanthology.org/2021.repl4nlp-1.11.pdf)                                                                                                      |
|   2021 | ACL      | [Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models](https://aclanthology.org/2021.repl4nlp-1.27.pdf)                 |
|   2021 | NAACL    | [Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation](https://aclanthology.org/2021.naacl-main.16.pdf)                                   |
|   2021 | NAACL    | [Multilingual BERT Post-Pretraining Alignment](https://aclanthology.org/2021.naacl-main.20.pdf)                                                                                              |
|   2021 | NAACL    | [mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer](https://aclanthology.org/2021.naacl-main.41.pdf)                                                                        |
|   2021 | NAACL    | [Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?](https://aclanthology.org/2021.naacl-main.73.pdf)                                                                             |
|   2021 | NAACL    | [LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval](https://aclanthology.org/2021.naacl-main.77.pdf)                                                  |
|   2021 | NAACL    | [Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning](https://aclanthology.org/2021.naacl-main.83.pdf)                    |
|   2021 | NAACL    | [Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach](https://aclanthology.org/2021.naacl-main.84.pdf)                            |
|   2021 | NAACL    | [Structure-Grounded Pretraining for Text-to-SQL](https://aclanthology.org/2021.naacl-main.105.pdf)                                                                                           |
|   2021 | NAACL    | [Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models](https://aclanthology.org/2021.naacl-main.108.pdf)                                               |
|   2021 | NAACL    | [ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding](https://aclanthology.org/2021.naacl-main.136.pdf)                              |
|   2021 | NAACL    | [Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models](https://aclanthology.org/2021.naacl-main.137.pdf)                                        |
|   2021 | NAACL    | [SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding](https://aclanthology.org/2021.naacl-main.152.pdf)                                                              |
|   2021 | NAACL    | [A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models](https://aclanthology.org/2021.naacl-main.162.pdf)                                         |
|   2021 | NAACL    | [Rethinking Network Pruning – under the Pre-train and Fine-tune Paradigm](https://aclanthology.org/2021.naacl-main.188.pdf)                                                                  |
|   2021 | NAACL    | [Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers](https://aclanthology.org/2021.naacl-main.189.pdf)                             |
|   2021 | NAACL    | [Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models](https://aclanthology.org/2021.naacl-main.195.pdf)                                      |
|   2021 | NAACL    | [Unified Pre-training for Program Understanding and Generation](https://aclanthology.org/2021.naacl-main.211.pdf)                                                                            |
|   2021 | NAACL    | [TABBIE: Pretrained Representations of Tabular Data](https://aclanthology.org/2021.naacl-main.270.pdf)                                                                                       |
|   2021 | NAACL    | [Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training](https://aclanthology.org/2021.naacl-main.278.pdf)                                     |
|   2021 | NAACL    | [InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training](https://aclanthology.org/2021.naacl-main.280.pdf)                                                |
|   2021 | NAACL    | [Context-Interactive Pre-Training for Document Machine Translation](https://aclanthology.org/2021.naacl-main.281.pdf)                                                                        |
|   2021 | NAACL    | [Cross-lingual Cross-modal Pretraining for Multimodal Retrieval](https://aclanthology.org/2021.naacl-main.285.pdf)                                                                           |
|   2021 | NAACL    | [Discourse Probing of Pretrained Language Models](https://aclanthology.org/2021.naacl-main.301.pdf)                                                                                          |
|   2021 | NAACL    | [Self-Alignment Pretraining for Biomedical Entity Representations](https://aclanthology.org/2021.naacl-main.334.pdf)                                                                         |
|   2021 | NAACL    | [Progressive Generation of Long Text with Pretrained Language Models](https://aclanthology.org/2021.naacl-main.341.pdf)                                                                      |
|   2021 | NAACL    | [Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality](https://aclanthology.org/2021.naacl-main.357.pdf)                         |
|   2021 | NAACL    | [Constructing Taxonomies from Pretrained Language Models](https://aclanthology.org/2021.naacl-main.373.pdf)                                                                                  |
|   2021 | NAACL    | [SCRIPT: Self-Critic PreTraining of Transformers](https://aclanthology.org/2021.naacl-main.409.pdf)                                                                                          |
|   2021 | NAACL    | [Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions](https://aclanthology.org/2021.naacl-main.420.pdf)                                                       |
|   2021 | NAACL    | [Self-training Improves Pre-training for Natural Language Understanding](https://aclanthology.org/2021.naacl-main.426.pdf)                                                                   |
|   2021 | NAACL    | [Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects](https://aclanthology.org/2021.naacl-main.430.pdf)                          |
|   2021 | NAACL    | [Pre-training with Meta Learning for Chinese Word Segmentation](https://aclanthology.org/2021.naacl-main.436.pdf)                                                                            |
|   2021 | NAACL    | [Shuffled-token Detection for Refining Pre-trained RoBERTa](https://aclanthology.org/2021.naacl-srw.12.pdf)                                                                                  |
|   2021 | NAACL    | [Pretrained Transformers for Text Ranking: BERT and Beyond](https://aclanthology.org/2021.naacl-tutorials.1.pdf)                                                                             |
|   2021 | NAACL    | [Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting](https://aclanthology.org/2021.naacl-industry.5.pdf)                                             |
|   2021 | NAACL    | [Low-Resource Machine Translation Using Cross-Lingual Language Model Pretraining](https://aclanthology.org/2021.americasnlp-1.26.pdf)                                                        |
|   2021 | NAACL    | [BioELECTRA:Pretrained Biomedical text Encoder using Discriminators](https://aclanthology.org/2021.bionlp-1.16.pdf)                                                                          |
|   2021 | NAACL    | [Improving Biomedical Pretrained Language Models with Knowledge](https://aclanthology.org/2021.bionlp-1.20.pdf)                                                                              |
|   2021 | NAACL    | [EntityBERT: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain](https://aclanthology.org/2021.bionlp-1.21.pdf)                                                   |
|   2021 | NAACL    | [The Effect of Pretraining on Extractive Summarization for Scientific Documents](https://aclanthology.org/2021.sdp-1.9.pdf)                                                                  |
|   2021 | NAACL    | [Unsupervised document summarization using pre-trained sentence embeddings and graph centrality](https://aclanthology.org/2021.sdp-1.14.pdf)                                                 |
|   2021 | NAACL    | [Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Text-based Translation](https://aclanthology.org/2021.sigtyp-1.7.pdf) |
|   2021 | NAACL    | [Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications](https://aclanthology.org/2021.smm4h-1.8.pdf)                                   |
|   2021 | NAACL    | [System description for ProfNER - SMMH: Optimized finetuning of a pretrained transformer and word vectors](https://aclanthology.org/2021.smm4h-1.11.pdf)                                     |
|   2021 | EACL     | [Dictionary-based Debiasing of Pre-trained Word Embeddings](https://aclanthology.org/2021.eacl-main.16.pdf)                                                                                  |
|   2021 | EACL     | [Non-Autoregressive Text Generation with Pre-trained Language Models](https://aclanthology.org/2021.eacl-main.18.pdf)                                                                        |
|   2021 | EACL     | [Self-Training Pre-Trained Language Models for Zero- and Few-Shot Multi-Dialectal Arabic Sequence Labeling](https://aclanthology.org/2021.eacl-main.65.pdf)                                  |
|   2021 | EACL     | [Combining Deep Generative Models and Multi-lingual Pretraining for Semi-supervised Document Classification](https://aclanthology.org/2021.eacl-main.76.pdf)                                 |
|   2021 | EACL     | [Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models](https://aclanthology.org/2021.eacl-main.95.pdf)                                            |
|   2021 | EACL     | [Debiasing Pre-trained Contextualised Embeddings](https://aclanthology.org/2021.eacl-main.107.pdf)                                                                                           |
|   2021 | EACL     | [Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models](https://aclanthology.org/2021.eacl-main.110.pdf)                                                           |
|   2021 | EACL     | [Cross-lingual Visual Pre-training for Multimodal Machine Translation](https://aclanthology.org/2021.eacl-main.112.pdf)                                                                      |
|   2021 | EACL     | [Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates](https://aclanthology.org/2021.eacl-main.145.pdf)                                      |
|   2021 | EACL     | [Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models](https://aclanthology.org/2021.eacl-main.190.pdf)                                                |
|   2021 | EACL     | [Structural Encoding and Pre-training Matter: Adapting BERT for Table-Based Fact Verification](https://aclanthology.org/2021.eacl-main.201.pdf)                                              |
|   2021 | EACL     | [Do Syntax Trees Help Pre-trained Transformers Extract Information?](https://aclanthology.org/2021.eacl-main.228.pdf)                                                                        |
|   2021 | EACL     | [Progressively Pretrained Dense Corpus Index for Open-Domain Question Answering](https://aclanthology.org/2021.eacl-main.244.pdf)                                                            |
|   2021 | EACL     | [Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees](https://aclanthology.org/2021.eacl-main.262.pdf)                                                                         |
|   2021 | EACL     | [Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](https://aclanthology.org/2021.eacl-main.284.pdf)                                                     |
|   2021 | EACL     | [Recipes for Adapting Pre-trained Monolingual and Multilingual Models to Machine Translation](https://aclanthology.org/2021.eacl-main.301.pdf)                                               |
|   2021 | EACL     | [A Little Pretraining Goes a Long Way: A Case Study on Dependency Parsing Task for Low-resource Morphologically Rich Languages](https://aclanthology.org/2021.eacl-srw.16.pdf)               |
|   2021 | EACL     | [Making Use of Latent Space in Language GANs for Generating Diverse Text without Pre-training](https://aclanthology.org/2021.eacl-srw.23.pdf)                                                |
|   2021 | EACL     | [Multidomain Pretrained Language Models for Green NLP](https://aclanthology.org/2021.adaptnlp-1.1.pdf)                                                                                       |
|   2021 | EACL     | [Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer](https://aclanthology.org/2021.adaptnlp-1.23.pdf)                                                             |
|   2021 | EACL     | [Few-Shot Learning of an Interleaved Text Summarization Model by Pretraining with Synthetic Data](https://aclanthology.org/2021.adaptnlp-1.24.pdf)                                           |
|   2021 | EACL     | [HerBERT: Efficiently Pretrained Transformer-based Language Model for Polish](https://aclanthology.org/2021.bsnlp-1.1.pdf)                                                                   |
|   2021 | EACL     | [Task-Specific Pre-Training and Cross Lingual Transfer for Sentiment Analysis in Dravidian Code-Switched Languages](https://aclanthology.org/2021.dravidianlangtech-1.9.pdf)                 |
|   2021 | EACL     | [Ask2Transformers: Zero-Shot Domain labelling with Pretrained Language Models](https://aclanthology.org/2021.gwc-1.6.pdf)                                                                    |
|   2021 | EACL     | [The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models](https://aclanthology.org/2021.wanlp-1.10.pdf)                                                          |
|   2021 | EACL     | [AraELECTRA: Pre-Training Text Discriminators for Arabic Language Understanding](https://aclanthology.org/2021.wanlp-1.20.pdf)                                                               |
|   2021 | EACL     | [AraGPT2: Pre-Trained Transformer for Arabic Language Generation](https://aclanthology.org/2021.wanlp-1.21.pdf)                                                                              |
|   2021 | Findings | [REPT: Bridging Language Models and Machine Reading Comprehension via Retrieval-Based Pre-training](https://aclanthology.org/2021.findings-acl.13.pdf)                                       |
|   2021 | Findings | [AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization](https://aclanthology.org/2021.findings-acl.37.pdf)                                                                    |
|   2021 | Findings | [Adapt-and-Distill: Developing Small, Fast and Effective Pretrained Language Models for Domains](https://aclanthology.org/2021.findings-acl.40.pdf)                                          |
|   2021 | Findings | [Weakly Supervised Pre-Training for Multi-Hop Retriever](https://aclanthology.org/2021.findings-acl.62.pdf)                                                                                  |
|   2021 | Findings | [MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training](https://aclanthology.org/2021.findings-acl.70.pdf)                                                                   |
|   2021 | Findings | [Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing](https://aclanthology.org/2021.findings-acl.100.pdf)                                                        |
|   2021 | Findings | [LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization](https://aclanthology.org/2021.findings-acl.119.pdf)                                                          |
|   2021 | Findings | [K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters](https://aclanthology.org/2021.findings-acl.121.pdf)                                                                    |
|   2021 | Findings | [Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models](https://aclanthology.org/2021.findings-acl.136.pdf)                                                            |
|   2021 | Findings | [Multi-Granularity Contrasting for Cross-Lingual Pre-Training](https://aclanthology.org/2021.findings-acl.149.pdf)                                                                           |
|   2021 | Findings | [A Comparison between Pre-training and Large-scale Back-translation for Neural Machine Translation](https://aclanthology.org/2021.findings-acl.150.pdf)                                      |
|   2021 | Findings | [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://aclanthology.org/2021.findings-acl.188.pdf)                                      |
|   2021 | Findings | [Correcting Chinese Spelling Errors with Phonetic Pre-training](https://aclanthology.org/2021.findings-acl.198.pdf)                                                                          |
|   2021 | Findings | [Structure-Aware Pre-Training for Table-to-Text Generation](https://aclanthology.org/2021.findings-acl.200.pdf)                                                                              |
|   2021 | Findings | [Dialogue-oriented Pre-training](https://aclanthology.org/2021.findings-acl.235.pdf)                                                                                                         |
|   2021 | Findings | [Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model](https://aclanthology.org/2021.findings-acl.237.pdf)                                                 |
|   2021 | Findings | [Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation](https://aclanthology.org/2021.findings-acl.239.pdf)                                            |
|   2021 | Findings | [MergeDistill: Merging Language Models using Pre-trained Distillation](https://aclanthology.org/2021.findings-acl.254.pdf)                                                                   |
|   2021 | Findings | [Exploring Unsupervised Pretraining Objectives for Machine Translation](https://aclanthology.org/2021.findings-acl.261.pdf)                                                                  |
|   2021 | Findings | [Probing Pre-Trained Language Models for Disease Knowledge](https://aclanthology.org/2021.findings-acl.266.pdf)                                                                              |
|   2021 | Findings | [Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice](https://aclanthology.org/2021.findings-acl.287.pdf)                                 |
|   2021 | Findings | [Multilingual Translation from Denoising Pre-Training](https://aclanthology.org/2021.findings-acl.304.pdf)                                                                                   |
|   2021 | Findings | [Probing Multi-modal Machine Translation with Pre-trained Language Model](https://aclanthology.org/2021.findings-acl.323.pdf)                                                                |
|   2021 | Findings | [Investigating Transfer Learning in Multilingual Pre-trained Language Models through Chinese Natural Language Inference](https://aclanthology.org/2021.findings-acl.331.pdf)                 |
|   2021 | Findings | [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://aclanthology.org/2021.findings-acl.334.pdf)                                    |
|   2021 | Findings | [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding](https://aclanthology.org/2021.findings-acl.370.pdf)                                                           |
|   2021 | Findings | [On the Copying Behaviors of Pre-Training for Neural Machine Translation](https://aclanthology.org/2021.findings-acl.373.pdf)                                                                |
|   2021 | Findings | [One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers](https://aclanthology.org/2021.findings-acl.387.pdf)                                                  |
|   2021 | Findings | [An Investigation of Suitability of Pre-Trained Language Models for Dialogue Generation – Avoiding Discrepancies](https://aclanthology.org/2021.findings-acl.393.pdf)                        |
|   2021 | Findings | [Learning to Sample Replacements for ELECTRA Pre-Training](https://aclanthology.org/2021.findings-acl.394.pdf)                                                                               |
|   2021 | Findings | [Task-adaptive Pre-training of Language Models with Word Embedding Regularization](https://aclanthology.org/2021.findings-acl.398.pdf)                                                       |
|   2021 | Findings | [Using Social and Linguistic Information to Adapt Pretrained Representations for Political Perspective Identification](https://aclanthology.org/2021.findings-acl.401.pdf)                   |


## Licenses

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Zhihong Chen](https://github.com/zhjohnchan) has waived all copyright and related or neighboring rights to this work.
